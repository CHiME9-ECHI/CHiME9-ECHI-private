# Baseline systems for the ECHI task of the CHiME-9 challenge

## Sections

1. <a href="#install">Installing the software</a>
2. <a href="#data">Installing the dataset</a>
3. <a href="#stages"> Running the baseline</a>
4. <a href="#troubleshooting">Troubleshooting</a>

## <a id="#install">1. Installing the software</a>

Clone this repository from GitHub

```bash
git clone git@github.com:CHiME9-ECHI/CHiME9-ECHI.git
cd CHiME9-ECHI
```

The installation of the necessary tools is detailed in `install.sh`.
We recommend to follow it step-by-step and adjust for your system if needed.
The script will build a conda environment called `echi_recipe`

```bash
install.sh
```

When running the system, remember to activate the conda environment and set the
necessary environment variables,

```bash
conda activate echi_recipe
export PYTHONPATH=$PWD/src:$PYTHONPATH
```

To make the `PYTHONPATH` setting persistent across terminal sessions, you can add
 the `export` command to your shell's configuration file (e.g., `~/.bashrc` for
 bash or `~/.zshrc` for zsh).

## <a id="data"> 2. Installing the data </a>

[Specific instructions on how to download and install the CHiME-9 ECHi dataset will
be provided here once available.]

The baseline system expects the dataset to be placed in the `data/chime9_echi`
 directory by default. The paths to various subsets of the data (e.g., training,
 development, evaluation) are defined in the `config/paths.yaml` file. If you
 choose to place the data in a different location, you will need to update
 `config/paths.yaml` accordingly. However, adhering to the default directory
 structure (`data/chime9_echi`) is recommended for ease of use.

## <a id="stages">3. Stages</a>

This repository is set up to handle all phases of training, enhancement and evaluation.
 Each of these has it's own pipeline, which will prepare the data and perform the
 intended task.

- **Train:** Prepares speech segments of the dataset and then trains using them.
 Details can be found on the [training page](docs/training.md).
- **Enhancement:** Given a system, the enhancement pipeline produces
 audio for each full session and saves it with the correct formatting. Default
 options for passthrough and the baseline are provided, but custom options
 can be added. Details can be found on the
 [enhancement page](docs/enhancement.md).
- **Evaluation:** Given a directory containing all the enhanced files, this
 script computes all the specified metrics over all sessions. Details can be
 found on the [evaluation page](docs/evaluation.md).

## <a id="troubleshooting">4. Troubleshooting</a>

If you encounter issues, here are some common troubleshooting steps:

- **Activate Conda Environment:** Ensure your Conda environment (`echi_recipe`) is
 activated:

  ```bash
  conda activate echi_recipe
  ```

- **Check PYTHONPATH:** Verify that your `PYTHONPATH` environment variable is correctly
 set to include the `src` directory of this project:

  ```bash
  export PYTHONPATH=$PWD/src:$PYTHONPATH
  # (or ensure this is in your .bashrc or equivalent shell startup script)
  echo $PYTHONPATH
  ```

- **Verify Data Paths:** Double-check that the dataset paths in `config/paths.yaml`
 match the actual location of your CHiME-9 ECHi data. The default expected location
 is `data/chime9_echi`.
- **Hydra Log Files:** For detailed error messages and execution logs, inspect the
 Hydra log files. These are typically found in the `exp/<experiment_name>/hydra/`
 directory (e.g., `exp/main/hydra/`). The exact path will be printed at the start
 of a run.
- **Common Python Issues:** Check for common Python package installation problems
 or version conflicts within the Conda environment. Sometimes, reinstalling a
 problematic package can help.

### Running Evaluation in Batches

The following command is an example of how to run the evaluation stage in parallel
batches using Hydra's multirun feature and using either Hydra's submitit job launcher
plugin.

For running on a local machine with multiple cores,

```bash
python run.py evaluate.n_batches=10 evaluate.batch='range(1,11)' \
 hydra/launcher=echi_submitit_local  --multirun
```

For running on an HPC facility with a Slurm scheduler

```bash
python run.py evaluate.n_batches=200 evaluate.batch='range(1,201)' \
 hydra/launcher=echi_submitit_slurm  --multirun
```

- `evaluate.n_batches=10`: This parameter informs the script that the data should
 be conceptually divided into 10 batches.
- `evaluate.batch='range(1,10)'`: This specific Hydra syntax tells the system to
 launch multiple runs, iterating through the values generated by `range(1,10)`.
 In Python, `range(1,10)` produces numbers from 1 up to (but not including) 10,
 so this will create runs for batch numbers 1, 2, 3, 4, 5, 6, 7, 8, and 9. Each of
 these runs will process its corresponding segment of the data.
- `--multirun`: This is a Hydra flag that enables launching multiple jobs based on
 the sweep defined by `evaluate.batch`. These jobs may run sequentially or in
 parallel, depending on your Hydra launcher configuration (e.g., basic local
 launcher vs. a Slurm or other HPC scheduler launcher).

**Note on batch numbering:** If you intend to process all 10 batches, numbered for
 example from 1 to 10, you would use `evaluate.batch='range(1,11)'`.

If using an HPC facilty and Slurm, please check the configuration file
 `config/hydra/launcher/echi_submitit_slurm.yaml` and edit to fit your system.
