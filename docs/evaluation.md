# Evaluation

The evaluation stage can be run on a given set of signals can be run with the
command

```bash
python run_evaluation.py
```

This is equivalent to running the following steps

```bash
python -m scripts.setup
python -m scripts.enhance
python -m scripts.validate
python -m scripts.prepare
python -m scripts.evaluate
python -m scripts.report
```

Results will appear in the reports directory defined in
`config/evaluation/main.yaml`. Results are reported at three levels:

- The device level, `report.dev.<device>._._.json` - i.e. accumulated over all
 sessions.
- The session level, `report.dev.<device>.<session>._.json` - i.e. for a specific
 session and given device.
- The participant level, `report.dev.<device>.<session>.<PID>.json` - i.e. for a
 specific participant within a session for a given device.

For the `dev` set there will be 2, 20 (2 devices x 10 session) and 60 (2 devices
 x 10 session x 3 participants) of these files respectively.

The reports are stored as a dictionary with an entry for each metric. Each metric,
in turn, is presented as a dictionary storing the `mean`, `standard deviation`,
`standard error`, `min value`, `max value`, and the `number of segments`.

For each `json` file there will also be a similarly named `csv` file containing
the metric data on which the statistics were computed.

## Configuring the baseline

The main configuration files are located in the `config/evaluation` directory:

- `main.yaml`: Main configuration, imports other specific configurations.
- `setup.yaml`: Configuration for the data setup stage
(`scripts/evaluation/setup.py`).
- `enhance.yaml`: Configuration for the enhancement stage
(`scripts/evaluation/enhance.py`).
- `validate.yaml`: Configuration for the validate stage
(`scripts/evaluation/validate.py`).
- `prepare.yaml`: Configuration for the preparation stage
(`scripts/evaluation/prepare.py`).
- `evaluate.yaml`: Configuration for the evaluation stage
(`scripts/evaluation/evaluate.py`).
- `report.yaml`: Configuration for the reporting stage
(`scripts/evaluation/report.py`).
- `metrics.yaml`: Configuration for the metrics used in evaluation.

This phase also relies on configurations which persist across all stages,
stored in the `config` directory.

- `paths.yaml`: Defines paths for data, models, and outputs.
- `shared.yaml`: Shared parameters used across different scripts (e.g., dataset paths,
general settings).

You can override any configuration parameter from the command line.

For `run_evaluation.py`, which executes the entire pipeline:

```bash
# Example: Run with a specific dataset configuration and disable GPU usage
# for enhancement
python run_evaluation.py dataset=dev enhance.use_gpu=false
```

For individual scripts like `scripts/evaluate.py`:

```bash
# Example: Evaluate a specific submission directory
python scripts/evaluate.py evaluate.enhanced_dir=<submission_dir>

# Example: Evaluate with specific test data
python scripts/evaluate.py evaluate.enhanced_dir=data/submission
```

Key configurable parameters include:

- **Dataset:** `dataset` allows you to specify different dataset configurations.
- **Device Settings:** Parameters like `enhance.use_gpu` (true/false) and
 `enhance.device` (e.g., 'cuda:0', 'cpu') control hardware usage.
- **Evaluation:**
  - `evaluate.submission`: Path to the enhanced audio or transcriptions to be evaluated.
  - `evaluate.n_batches`, `evaluate.batch`: Control parallel processing during
 evaluation by splitting the data into batches.

## Running Evaluation in Batches

The following command is an example of how to run the evaluation stage in parallel
batches using Hydra's multirun feature and using either Hydra's submitit job launcher
plugin.

For running on a local machine with multiple cores,

```bash
python run_evaluation.py evaluate.n_batches=10 evaluate.batch='range(1,11)' \
 hydra/launcher=echi_submitit_local  --multirun
```

For running on an HPC facility with a Slurm scheduler

```bash
python run_evaluation.py evaluate.n_batches=200 evaluate.batch='range(1,201)' \
 hydra/launcher=echi_submitit_slurm  --multirun
```

- `evaluate.n_batches=10`: This parameter informs the script that the data should
 be conceptually divided into 10 batches.
- `evaluate.batch='range(1,10)'`: This specific Hydra syntax tells the system to
 launch multiple runs, iterating through the values generated by `range(1,10)`.
 In Python, `range(1,10)` produces numbers from 1 up to (but not including) 10,
 so this will create runs for batch numbers 1, 2, 3, 4, 5, 6, 7, 8, and 9. Each of
 these runs will process its corresponding segment of the data.
- `--multirun`: This is a Hydra flag that enables launching multiple jobs based on
 the sweep defined by `evaluate.batch`. These jobs may run sequentially or in
 parallel, depending on your Hydra launcher configuration (e.g., basic local
 launcher vs. a Slurm or other HPC scheduler launcher).

**Note on batch numbering:** If you intend to process all 10 batches, numbered for
 example from 1 to 10, you would use `evaluate.batch='range(1,11)'`.

If using an HPC facilty and Slurm, please check the configuration file
 `config/hydra/launcher/echi_submitit_slurm.yaml` and edit to fit your system.
